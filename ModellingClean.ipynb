{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "mpl.style.available\n",
    "mpl.style.use('ggplot') \n",
    "from IPython.display import display, HTML\n",
    "import datetime as dt \n",
    "from IPython.display import clear_output\n",
    "import sklearn\n",
    "\n",
    "from patsy import dmatrices\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from time import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set score measure here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score_measure = 'recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_score_clf(clf, X, y, n_cv):\n",
    "\n",
    "    accuracy = cross_val_score(clf, X, y, cv=n_cv,n_jobs=-1)\n",
    "    precision = cross_val_score(clf, X, y, cv=n_cv, scoring='precision',n_jobs=-1)\n",
    "    recall = cross_val_score(clf, X, y, cv=n_cv, scoring='recall',n_jobs=-1)\n",
    "\n",
    "    print('Report for', clf.__class__.__name__)\n",
    "    print('')\n",
    "    print('accuracy: ', accuracy.mean())\n",
    "    print('precision: ', precision.mean())\n",
    "    print('recall: ', recall.mean())\n",
    "    \n",
    "    f1 = 2 * (precision.mean()*recall.mean())/(precision.mean() + recall.mean())\n",
    "    \n",
    "    print('f1: ', f1)\n",
    "    print('')\n",
    "    \n",
    "    return clf, accuracy.mean(), precision.mean(), recall.mean(), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_clf(clf, X, y, n_cv):\n",
    "\n",
    "    accuracy = cross_val_score(clf, X, y, cv=n_cv,n_jobs=-1)\n",
    "    precision = cross_val_score(clf, X, y, cv=n_cv, scoring='precision',n_jobs=-1)\n",
    "    recall = cross_val_score(clf, X, y, cv=n_cv, scoring='recall',n_jobs=-1)\n",
    "\n",
    "    fitted_clfs.append(clf)\n",
    "    \n",
    "\n",
    "    print('Report for', clf.__class__.__name__)\n",
    "    print('')\n",
    "    print('accuracy: ', accuracy.mean())\n",
    "    print('precision: ', precision.mean())\n",
    "    print('recall: ', recall.mean())\n",
    "    print('')\n",
    "    \n",
    "    return clf, accuracy.mean(), precision.mean(), recall.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_recusive_feature_eval_cv(clf, X, y, features):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    if(clf.__class__.__name__ == 'RandomForestClassifier'):\n",
    "        step = 2\n",
    "    else:\n",
    "        step = 1\n",
    "    \n",
    "    rfecv = RFECV(estimator=clf, step=step, scoring=score_measure, n_jobs=-1)\n",
    "    rfecv = rfecv.fit(X,y)\n",
    "    \n",
    "    print('RFECV ran in ', time() - start, 'secs for '+clf.__class__.__name__)\n",
    "    print(\"\")\n",
    "    print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    print(\"\")\n",
    "    print(\"Supporting features: \")\n",
    "    print(\"\")\n",
    "    print(score_measure+' score with optimal feature selection: ', rfecv.grid_scores_.max())\n",
    "    print(\"\")\n",
    "    \n",
    "    support_features = []\n",
    "    \n",
    "    for i in range(len(rfecv.support_)):\n",
    "    \n",
    "        if(rfecv.support_[i]):\n",
    "            \n",
    "            print(features[i])\n",
    "            \n",
    "            support_features.append(features[i])\n",
    "            \n",
    "    print(len(rfecv.grid_scores_))\n",
    "    print(len(range(step, X.shape[1] + step, step)))\n",
    "    \n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score\"+\"(\"+score_measure+\")\")\n",
    "    plt.plot(range(step, X.shape[1] + step, step), rfecv.grid_scores_)\n",
    "    plt.show()        \n",
    "    \n",
    "    return support_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_forest_feature_importance(forest,X,y,features):\n",
    "\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    \n",
    "    important_features = []\n",
    "\n",
    "    for f in range(X.shape[1]):\n",
    "        print(str(f+1)+'. '+features[indices[f]]+' '+str(importances[indices[f]]))\n",
    "        \n",
    "        important_features.append(features[indices[f]])\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"./customer_df_with_engineered_features.pkl\")\n",
    "df.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First run through classifiers with no optimization (feature or parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick some classifiers to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##  Logisitic regression, linear SVM, random forest\n",
    "clfs = [LogisticRegression(),\n",
    "        RandomForestClassifier(n_estimators = 150, max_depth = 15, max_features = 'sqrt'), \n",
    "        LinearSVC(tol = 0.01)]\n",
    "\n",
    "#Loop through each classifier and cross-validate, check accuracy, precision and recall\n",
    "\n",
    "y = df.churn_label\n",
    "\n",
    "y = np.ravel(y)\n",
    "\n",
    "## Drop non-feature columns\n",
    "X = df.drop(\n",
    "    ['customer_id', 'churn_label', 'gender', 'country', 'date_created',\n",
    "       'YOB', 'premier', 'account_age', 'account_age_months','rounded_total_spent'], axis = 1)\n",
    "\n",
    "features = X.columns\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "accuracy_l = []\n",
    "precision_l = []\n",
    "recall_l = []\n",
    "f1_score_l = []\n",
    "fitted_clfs = []\n",
    "\n",
    "#Cycle through classifiers\n",
    "\n",
    "for clf in clfs:\n",
    "    \n",
    "    clf_name = clf.__class__.__name__\n",
    "    \n",
    "    fitted_clf, accuracy, precision, recall, f1_score = fit_and_score_clf(clf, X, y, 5)\n",
    "    \n",
    "    accuracy_l.append((clf_name, accuracy))\n",
    "    precision_l.append((clf_name, precision))\n",
    "    recall_l.append((clf_name, recall))\n",
    "    f1_score_l.append((clf_name, f1_score))\n",
    "    \n",
    "    fitted_clfs.append(clf)\n",
    "    \n",
    "    ## Plot confusion matrix\n",
    "\n",
    "    class_names = ['not_churned','churned']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title=clf.__class__.__name__+', Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "print('Accuracy rankings: ')\n",
    "print(sorted(accuracy_l, key=lambda tup: tup[1], reverse = True))\n",
    "\n",
    "print('Precision rankings: ')\n",
    "print(sorted(precision_l, key=lambda tup: tup[1], reverse = True))\n",
    "\n",
    "print('Recall rankings: ')\n",
    "print(sorted(recall_l, key=lambda tup: tup[1], reverse = True))\n",
    "\n",
    "print('F1 score rankings: ')\n",
    "print(sorted(f1_score_l, key=lambda tup: tup[1], reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize features for scoring measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "support_features_l = []\n",
    "fitted = []\n",
    "\n",
    "#perform recursive feature evaluation using scoring measure\n",
    "for clf in clfs:\n",
    "    \n",
    "    support_features = perform_recusive_feature_eval_cv(clf, X, y, features)\n",
    "    \n",
    "    support_features_l.append((clf_name,support_features))\n",
    "\n",
    "##Plot random forest feature importance\n",
    "importance_sorted_features = \\\n",
    "    random_forest_feature_importance(clfs[1],X,y,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retest with optimized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(clfs)):\n",
    "\n",
    "    ## Drop non-feature columns\n",
    "    X = df[support_features_l[i][1]]\n",
    "\n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    clf = clfs[i]\n",
    "    \n",
    "    fitted_clf, accuracy, precision, recall, f1_score = fit_and_score_clf(clf, X, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use support features found above\n",
    "X = df[support_features_l[0][1]]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "GS = GridSearchCV(cv=None,\n",
    "       estimator=LogisticRegression(C=1.0,penalty='l2', max_iter = 500),\n",
    "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'class_weight' : [None,'balanced'],\n",
    "                  },scoring=score_measure, n_jobs = -1)\n",
    "\n",
    "\n",
    "GS.fit(X,y)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(GS.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = GS.cv_results_['mean_test_score']\n",
    "stds = GS.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, GS.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C=GS.best_params_['C'], class_weight=GS.best_params_['class_weight'], penalty='l2',\n",
    "                         tol=0.0001, solver = 'sag')\n",
    "\n",
    "fitted_clf, accuracy, precision, recall, f1_score = fit_and_score_clf(clf, X, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use support features found above\n",
    "X = df[support_features_l[1][1]]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "param_grid = {\"min_samples_split\": [2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "\n",
    "GS = GridSearchCV(cv=None,\n",
    "       estimator=RandomForestClassifier(n_estimators = 150, max_depth = 15, max_features = 'sqrt'),\n",
    "       param_grid=param_grid,scoring=score_measure, n_jobs = -1)\n",
    "\n",
    "\n",
    "GS.fit(X,y)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(GS.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = GS.cv_results_['mean_test_score']\n",
    "stds = GS.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, GS.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "clf =RandomForestClassifier(n_estimators = 150, max_depth = 15, max_features = 'sqrt',\n",
    "                            min_samples_split = GS.best_params_['min_samples_split'],\n",
    "                            min_samples_leaf = GS.best_params_['min_samples_leaf'],\n",
    "                            bootstrap = GS.best_params_['bootstrap'],\n",
    "                            criterion = GS.best_params_['criterion'])\n",
    "\n",
    "fitted_clf, accuracy, precision, recall, f1_score = fit_and_score_clf(clf, X, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use support features found above\n",
    "X = df[support_features_l[2][1]]\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "GS = GridSearchCV(cv=None,\n",
    "       estimator=LinearSVC(C=1.0),\n",
    "       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'class_weight' : [None,'balanced'],\n",
    "                  },scoring=score_measure, n_jobs = -1)\n",
    "\n",
    "\n",
    "GS.fit(X,y)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print()\n",
    "print(GS.best_params_)\n",
    "print()\n",
    "print(\"Grid scores on development set:\")\n",
    "print()\n",
    "means = GS.cv_results_['mean_test_score']\n",
    "stds = GS.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, GS.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "print()\n",
    "\n",
    "\n",
    "clf = LinearSVC(C=GS.best_params_['C'], class_weight=GS.best_params_['class_weight'])\n",
    "\n",
    "fitted_clf, accuracy, precision, recall, f1_score = fit_and_score_clf(clf, X, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
